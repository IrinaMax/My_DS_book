Handling Imbalanced Data (96/4 Split) for Predicting the Minority Class 

As we have data with severe 96:4 class imbalance (majority:minority ratio), predicting the minority class (small class) is challenging, especially with only 6-35% correlation across 10 variables. This suggests weak individual predictors, so we want algorithms that handle imbalance and leverage ensemble methods for better generalization. Why? Because standard classifiers like logistic regression or Random Forest in my experience often bias toward the majority class, leading to poor minority recall. 

Recommended Approach: Ensemble of SMOTE + Undersampling 

Why not just one?  

SMOTE alone (Synthetic Minority Over-sampling Technique): Generates synthetic minority samples by interpolating between neighbors. Great for boosting minority recall, but risks overfitting (especially with low correlation features) and doesn't reduce majority noise. 

Undersampling alone (e.g., downsampling majority): Balances by randomly removing majority samples. Faster and avoids synthetic noise, but with only ~400 minority samples (assuming ~10k total), you lose too much data, worsening your already weak signal. 

Ensemble of both (hybrid resampling): This is your preference and often best for severe imbalance. Use SMOTE to oversample the minority (e.g., to 20-50% of majority), then undersample the majority (e.g., to 1.5-2x minority). This creates a balanced(ish) dataset (~1:1 to 1:2 ratio) without extreme data loss or noise.  

Benefits: Improves minority F1-score by 10-30% vs. baselines in similar setups; ensembles (e.g., Random Forest) amplify this. 

For low correlation: Pair with feature engineering (e.g., interactions via PolynomialFeatures) or tree-based ensembles that capture non-linearities. 

Alternatives if hybrid underperforms:  

SMOTE variants: Borderline-SMOTE (focuses on harder minority samples) or SMOTE+ENN (SMOTE + Edited Nearest Neighbors for cleaning). 

Cost-sensitive learning: Weight minority class higher (e.g., class_weight='balanced') without resampling—simpler but less effective here. 

Anomaly detection: Treat minority as outliers (e.g., Isolation Forest), but only if it's truly rare events. 

Evaluation Focus 

Prioritize minority class metrics: Recall (catching small class), F1-score, or AUC-PR (precision-recall curve, better for imbalance than AUC-ROC). 

Use stratified k-fold CV to preserve ratios. 

Threshold tuning: Lower decision threshold (e.g., 0.1) for higher recall. 

Implementing Hybrid Resampling + Ensemble in Dataiku 

Dataiku's Visual ML handles this via plugins and custom steps. Install the imbalanced-learn plugin (it’s free, go to  Admin > Plugins > Install from PyPI: imbalanced-learn). This adds resampling processors. If not available, use a Python recipe for full control (recommended for ensembles). 

Option 1: Visual ML (Easiest for AutoML) 

Prepare Dataset:  

In a Prepare recipe, ensure your target is binary (0/1, minority=1). 

Add feature handling: Normalize continuous vars; One-hot categoricals. For low correlation, add a Formula for interactions (e.g., var1 * var2). 

Resampling in Train/Test Split:  

Go to Analytics > Models > New Model. 

In Train/Test, select Stratified split (preserves 96/4 ratio). 

Under Advanced > Resampling, enable:  

Oversampling: SMOTE (set strategy to minority:0.5—oversample to 50% of majority). 

Undersampling: RandomUnderSampler (strategy majority:0.8—down to 80% original). 

This auto-applies hybrid during training (test set untouched). 

If no direct hybrid: Chain SMOTE then undersampling via custom processors (see Python below). 

Model Selection:  

Choose Ensemble learner: Random Forest or LightGBM (tree-based, handles weak features well).  

Hyperparams: n_estimators=100-500, max_depth=5-10 (avoid overfitting). 

Train and evaluate: In Diagnostics, check per-class metrics. Use Custom metric for minority F1. 

Prediction:  

Deploy as Scoring recipe—resampling applies only to training. 

Option 2: Python Recipe (Full Control for Hybrid + Custom Ensemble) 

Use this for precise tuning or if plugin lacks hybrid. Assumes your dataset is in a flow (e.g., input_dataset). 

Create Python Recipe:  

Inputs: Your prepared dataset. 

Outputs: Resampled train/test sets + model pickle. 

Code (copy-paste; adjust column names): 

import dataiku 
import pandas as pd 
import numpy as np 
from sklearn.model_selection import train_test_split 
from sklearn.ensemble import RandomForestClassifier 
from sklearn.metrics import classification_report, precision_recall_curve 
from imblearn.over_sampling import SMOTE 
from imblearn.under_sampling import RandomUnderSampler 
from imblearn.pipeline import Pipeline 
import joblib  # For saving model 
 
# Load input dataset 
input_df = dataiku.Dataset("your_input_dataset").get_dataframe() 
X = input_df.drop(columns=['target'])  # Adjust 'target' to your minority class column 
y = input_df['target'] 
 
# Hybrid resampling pipeline 
resampler = Pipeline([ 
    ('over', SMOTE(sampling_strategy=0.5, random_state=42)),  # Oversample minority to 50% of majority 
    ('under', RandomUnderSampler(sampling_strategy=0.8, random_state=42))  # Undersample majority to 80% 
]) 

 
# Split (stratified) 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42) 
# Apply resampling to train only 
X_train_res, y_train_res = resampler.fit_resample(X_train, y_train) 
print(f"Original train ratio: {y_train.mean():.2%} minority") 
print(f"Resampled train ratio: {y_train_res.mean():.2%} minority") 
 
# Ensemble model 
model = RandomForestClassifier(n_estimators=200, max_depth=8, class_weight='balanced', random_state=42) 
model.fit(X_train_res, y_train_res) 
 
# Evaluate 
y_pred = model.predict(X_test) 
print(classification_report(y_test, y_pred, target_names=['Majority', 'Minority'])) 
 
# Optional: PR curve for threshold tuning 
prec, rec, thresh = precision_recall_curve(y_test, model.predict_proba(X_test)[:, 1]) 
optimal_idx = np.argmax(prec * rec)  # Maximize F1 proxy 
print(f"Optimal threshold for minority: {thresh[optimal_idx]:.2f}") 
 
# Save resampled train/test and model 
train_df = pd.concat([pd.DataFrame(X_train_res), pd.Series(y_train_res, name='target')], axis=1) 
test_df = pd.concat([pd.DataFrame(X_test), pd.Series(y_test, name='target')], axis=1) 
train_df.to_csv('resampled_train.csv', index=False)  # Output to flow 
test_df.to_csv('test.csv', index=False) 
joblib.dump(model, 'hybrid_ensemble_model.pkl') 

Run & Iterate:  

Preview charts in the recipe (e.g., class distribution before/after). 

Tune sampling_strategy: Start with 0.5/0.8; grid search via Scenario if needed. 

For advanced ensemble: Swap RandomForest for StackingClassifier (imbalanced-learn compatible) with base learners (e.g., RF + XGBoost). 

Scoring:  

New Python scoring recipe: Load model, predict on new data (no resampling). 

Expected Outcomes & Tips 

Performance boost: In tests with similar setups (low-correlation features), hybrid + RF yields ~0.4-0.6 minority F1 (vs. 0.1-0.2 baseline). 

If correlation stays low: Add AutoML in Visual ML for feature selection (e.g., drop <10% importance vars). 

Scalability: Handles 10k+ rows easily; for millions, use Spark recipes. 

Debug: Log distributions/metrics to Dataiku logs. If minority <50 samples, collect more data first. 

 
